日期： 2023-02-19

标签： #学习笔记 #技术

学习资料： 
[视频回放](https://pan.baidu.com/play/video#/video?path=%2F%E5%AD%A6%E4%B9%A0%2F%E8%B7%AF%E5%93%A5%E7%89%B9%E8%AE%AD%E8%90%A5%2F2023-02-19_Koom%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E5%92%8C%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93.mp4&t=-1)

---
<br>

Heap Dump
- Thread Stacks and Local Variables：栈回溯
Reachability
Shallow vs. Retained Heap
- Shallow heap
- Retained set
- Retained heep：即将被gc回收释放
Dominator Tree
Garbadge Collection Root

Koom给我们的总结和反思
- OOMTracker、FdOOMTracker、HeapOOMTracker、PhysicalMemoryOOMTracker、FastHugeMemoryOOMTracker、ThreadOOMTracker
- 为什么Dump需要在子进程，而非子线程？ForkjvmHeapDumper.java
- runtime/debugger.cc
- nm -a 

Bitmap优化
- 链路回收（统一图片库收敛、控制）
- 大图的监控、重复突破

内存的监控
- java内存（systarce、LeakCanary、MAT）
- Native内存
- OOM监控（Koom）

架构优化
- APM 
- 缓存机制（ComponetCallbacks2）
- 减少进程（空进程=10M）
- so的优化（自己写的so文件，编译时-finstrument-functions，解决内存泄漏问题）

核心指标
- 异常率：PSS超过400M次数 / 采集次数
- 触顶率：Java内存使用情况
- GC：getGloableAllocCount、getGloableAllocSize、getRuntimeStat("art.gc.gc-count") gc次数
- 内存事件：onTrimMemory()

GitHub：memory-leak-detector


# 路哥讲义

### 性能优化内存篇

  

#### 虚拟内存的内存模型

  

![img](https://p.ipic.vip/h8k5k5.png)

  

**在 Linux 系统中，存放操作系统的虚拟内存区域被称为内核空间，剩下的存放应用的虚拟内存区域称为用户空间。** 内核空间占用了 1G，位于虚拟地址的高地址区域，而 ELF 文件的一些数据，是存放在低地址的区域（即从地址 0 开始）。

  

1. 栈：由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。

2. 堆：动态内存分配，可以由开发者自己分配和释放（malloc 和 free 函数实现），Android 开发时不需要我们手动分配和释放，因为虚拟机程序已经帮我们做了。堆的开始地址由变量 start_brk 描述，堆的当前地址由变量 brk 描述。

3. BSS：存放全局未初始化，静态未初始化数据。

4. 数据段：存放全局初始化，静态初始化数据。

5. 程序代码区：存放的是 ELF 文件代码段。

  

#### 虚拟内存分配

  

~~~

void *malloc(size_t size)

  

void *mmap(void *addr,size_t length,int prot,int flags,int fd, off_t offset);

~~~

  

- 参数 addr 指向欲映射的内存起始地址，通常设为 NULL，代表让系统自动选定地址，映射成功后返回该地址；

  

- 参数 length 表示将文件中多大的部分映射到内存；

  

- 参数 prot 指定映射区域的读写权限；

  

- 参数 flags 指定映射时的特性，如是否允许其他进程映射这段内存；

  

- 参数 fd 指定映射内存的文件描述符；

  

- 参数 offset 指定映射位置的偏移量，一般为 0。

  

mmap 函数有 2 种用法：

  

1. 映射磁盘文件到用户空间中；

  

1. 匿名映射，不映射磁盘文件，而是向映射区申请一块内存，此时的 fd 入参传 -1。

  

第 1 种用法可以让我们读文件的效率更高（比如 Android 读取 dex 文件就是通过 mmap 来提高读取速度），也可以用来实现数据跨进程传输（比如 Android 共享内存机制、Binder 通信都是通过 mmap 来实现的）。malloc() 函数使用了 mmap 函数的第 2 种用法，即在 Heap 区域中申请一块内存。

  

#### 内存描述指标

  

- **PSS**（ Proportional Set Size ）：实际使用的物理内存，会按比例分配共享的内存。比如一个应用有两个进程都用到了 Chrome 的 V8 引擎，那么每个进程会承担 50% 的 V8 这个 so 库的内存占用。PSS 是我们使用最频繁的一个指标，App 线上的内存数据统计一般都取这个指标。

- **RSS**（ Resident Set Size ）：PSS 中的共享库会按比例分担，但是 RSS 不会，它会完全算进当前进程，所以把所有进程的 RSS 加总后得出来的内存会比实际高。按比例计算内存占用会有一定的消耗，因此当想要高性能的获取内存数据时便可以使用 RSS，Android 的 LowMemoryKiller 机制就是根据每个进程的 RSS 来计算进程优先级的。

- **Private Clean / Private Dirty**：当我们执行 dump meminfo 时会看到这个指标，Private 内存是只被当前进程独占的物理内存。独占的意思是即使释放之后也无法被其他进程使用，只有当这个进程销毁后其他进程才能使用。Clean 表示该对应的物理内存已经释放了，Dirty 表示对应的物理内存还在使用。

- **Swap Pss Dirty**：这个指标和上面的 Private 指标刚好相反，Swap 的内存被释放后，其他进程也可以继续使用，所以我们在 meminfo 中只看得到 Swap Pss Dirty，而看不到Swap Pss Clean，因为 Swap Pss Clean 是没有意义的。

- **Heap Alloc**：通过 Malloc、mmap 等函数实际申请的虚拟内存，包括 Naitve 和虚拟机申请的内存。

- **Heap Free**：空闲的虚拟内存。

  

~~~shell

generic_x86:/ $ dumpsys meminfo com.android.hybrid

Applications Memory Usage (in Kilobytes):

Uptime: 59526051 Realtime: 59526051

  

** MEMINFO in pid 11822 [com.android.hybrid] **

                   Pss  Private  Private  SwapPss     Heap      Heap     Heap

                 Total    Dirty    Clean    Dirty     Size    Alloc     Free

                ------   ------   ------   ------   ------   ------   ------

  Native Heap     7064     6984        0        0    29184    23672     5511

  Dalvik Heap     1299      780        0        0     2687     1151     1536

 Dalvik Other      463      444        0        0

        Stack      412      412        0        0

       Ashmem        5        0        0        0

    Other dev        9        0        8        0

     .so mmap    16892      496    14584        0

    .apk mmap      478        0        0        0

    .ttf mmap     1750        0       60        0

    .dex mmap     7072        4     5424        0

    .oat mmap      296        0        0        0

    .art mmap     3980     3760        0        0

   Other mmap    31997        4     1204        0

      Unknown    90648    90616        0        0

        TOTAL   162365   103500    21280        0    31871    24823     7047

  

 App Summary

                       Pss(KB)

                        ------

           Java Heap:     4540

         Native Heap:     6984

                Code:    20568

               Stack:      412

            Graphics:        0

       Private Other:    92276

              System:    37585

  

               TOTAL:   162365       TOTAL SWAP PSS:        0

  

 Objects

               Views:       16         ViewRootImpl:        1

         AppContexts:        3           Activities:        1

              Assets:        2        AssetManagers:        4

       Local Binders:        8        Proxy Binders:       14

       Parcel memory:        6         Parcel count:       24

    Death Recipients:        0      OpenSSL Sockets:        0

            WebViews:        0

  

 SQL

         MEMORY_USED:        0

  PAGECACHE_OVERFLOW:        0          MALLOC_SIZE:        0

  

generic_x86:/ $

  

###more /proc/meminfo

  

MemTotal:        1530616 kB [RAM大小]

MemFree:          224868 kB 【这里不是说这些内存都可以使用 只是说这些内存没有分配 】

MemAvailable:     888076 kB

Buffers:           15488 kB

Cached:           685568 kB

SwapCached:            0 kB

Active:           785160 kB

Inactive:         295444 kB

Active(anon):     381664 kB

Inactive(anon):     1660 kB

Active(file):     403496 kB

Inactive(file):   293784 kB

Unevictable:         256 kB

Mlocked:             256 kB

SwapTotal:             0 kB

SwapFree:              0 kB

Dirty:                 0 kB

Writeback:             0 kB

AnonPages:        379808 kB

Mapped:           448776 kB

Shmem:              3776 kB

Slab:              67116 kB

SReclaimable:      28400 kB

SUnreclaim:        38716 kB

KernelStack:       14112 kB

PageTables:        19228 kB

NFS_Unstable:          0 kB

Bounce:                0 kB

WritebackTmp:          0 kB

CommitLimit:      765308 kB

Committed_AS:   26973244 kB

VmallocTotal:   34359738367 kB

VmallocUsed:       86888 kB

VmallocChunk:   34359582724 kB

HugePages_Total:       0

HugePages_Free:        0

HugePages_Rsvd:        0

HugePages_Surp:        0

Hugepagesize:       2048 kB

DirectMap4k:       14168 kB

DirectMap2M:     1558528 kB

~~~

  

http://aospxref.com/android-10.0.0_r47/xref/frameworks/base/core/jni/android_os_Debug.cpp

  

**内存文件精髓：**

  

/maps文件

  

~~~

address | perms | offset | dev | inode | pathname

f401c000-f401d000 rw-p 00000000 00:00 0                                  [anon:linker_alloc]

f401d000-f401e000 r--p 00000000 00:00 0                                  [anon:linker_alloc]

f401e000-f4020000 rw-p 00000000 00:00 0                                  [anon:linker_alloc_vector]

f4020000-f4024000 rw-p 00000000 00:00 0                                  [anon:linker_alloc_small_objects]

f4024000-f4026000 r--p 00000000 00:00 0                                  [anon:linker_alloc]

f4026000-f4027000 rw-p 00000000 00:00 0                                  [anon:linker_alloc_vector]

f4027000-f4028000 rw-p 00000000 00:00 0                                  [anon:linker_alloc_small_objects]

f4028000-f4029000 r--p 00000000 00:00 0                                  [anon:linker_alloc]

f4029000-f4049000 r--s 00000000 00:10 7234                               /dev/__properties__/u:object_r:debug_prop:s0

f4049000-f4069000 r--s 00000000 00:10 7239                               /dev/__properties__/properties_serial

f4069000-f406b000 rw-p 00000000 00:00 0                                  [anon:linker_alloc_small_objects]

f406b000-f406c000 rw-p 00000000 00:00 0                                  [anon:linker_alloc_vector]

~~~

  

(1)地址：本段在虚拟内存中的地址范围;对应vm_area_struct中的vm_start和vm_end。

(2)权限：本段的权限; r-读，w-写，x-执行， p-私有;对应vm_flags。

(3)偏移地址：即本段映射地址在文件中的偏移;对于有名映射指本段映射地址在文件中的偏移,对应vm_pgoff；对于匿名映射为vm_area_struct->vm_start。

(4)主设备号与次设备号：所映射的文件所属设备的设备号，对应vm_file->f_dentry->d_inode->i_sb->s_dev。匿名映射为0。其中fd为主设备号，00为次设备号。

(5)文件索引节点号：对应vm_file->f_dentry->d_inode->i_ino，与ls –i显示的内容相符。匿名映射为0。;

(6)映射的文件名：对有名映射而言，是映射的文件名，对匿名映射来说，是此段内存在进程中的作用。[stack]表示本段内存作为栈来使用，[heap]作为堆来使用，其他情况则为无。

  

###### 进程地址空间划分

  

![img](https://p.ipic.vip/471oov.jpg)

  

- 代码段(text)

  

代码段也称正文段或文本段，通常用于存放程序执行代码(即CPU执行的机器指令)。一般C语言执行语句都编译成机器代码保存在代码段。通常代码段是可共享的，因此频繁执行的程序只需要在内存中拥有一份拷贝即可。

  

代码段通常属于只读，以防止其他程序意外地修改其指令(对该段的写操作将导致段错误)。某些架构也允许代码段为可写，即允许修改程序。

  

代码段指令根据程序设计流程依次执行，对于顺序指令，只会执行一次(每个进程)；若有反复，则需使用跳转指令；若进行递归，则需要借助栈来实现。

  

代码段指令中包括操作码和操作对象(或对象地址引用)。若操作对象是立即数(具体数值)，将直接包含在代码中；若是局部数据，将在栈区分配空间，然后引用该数据地址；若位于BSS段和数据段，同样引用该数据地址。

  

代码段最容易受优化措施影响。

  

- 数据段(Data)

  

**数据段通常用于存放程序中已初始化且初值不为0的全局变量和静态局部变量。数据段属于静态内存分配(静态存储区)，可读可写。**

  

数据段保存在目标文件中(在嵌入式系统里一般固化在镜像文件中)，其内容由程序初始化。例如，对于全局变量int gVar = 10，必须在目标文件数据段中保存10这个数据，然后在程序加载时复制到相应的内存。

  

**数据段与BSS段的区别如下：**

  

1、BSS段不占用物理文件尺寸，但占用内存空间；数据段占用物理文件，也占用内存空间。

  

对于大型数组如int ar0[10000] = {1, 2, 3, ...}和int ar1[10000]，ar1放在BSS段，只记录共有10000*4个字节需要初始化为0，而不是像ar0那样记录每个数据1、2、3...，此时BSS为目标文件所节省的磁盘空间相当可观。

  

2、当程序读取数据段的数据时，系统会出发缺页故障，从而分配相应的物理内存；当程序读取BSS段的数据时，内核会将其转到一个全零页面，不会发生缺页故障，也不会为其分配相应的物理内存。

  

运行时数据段和BSS段的整个区段通常称为数据区。某些资料中“数据段”指代数据段 + BSS段 + 堆。

  

- BSS段

  

 BSS(Block Started by Symbol)段中通常存放程序中以下符号：

  

- 未初始化的全局变量和静态局部变量

- 初始值为0的全局变量和静态局部变量(依赖于编译器实现)

- 未定义且初值不为0的符号(该初值即common block的大小)

  

C语言中，未显式初始化的静态分配变量被初始化为0(算术类型)或空指针(指针类型)。由于程序加载时，BSS会被操作系统清零，所以未赋初值或初值为0的全局变量都在BSS中。BSS段仅为未初始化的静态分配变量预留位置，在目标文件中并不占据空间，这样可减少目标文件体积。但程序运行时需为变量分配内存空间，故目标文件必须记录所有未初始化的静态分配变量大小总和(通过start_bss和end_bss地址写入机器代码)。当加载器(loader)加载程序时，将为BSS段分配的内存初始化为0。在嵌入式软件中，进入main()函数之前BSS段被C运行时系统映射到初始化为全零的内存(效率较高)。

  

注意，尽管均放置于BSS段，但初值为0的全局变量是强符号，而未初始化的全局变量是弱符号。若其他地方已定义同名的强符号(初值可能非0)，则弱符号与之链接时不会引起重定义错误，但运行时的初值可能并非期望值(会被强符号覆盖)。因此，定义全局变量时，若只有本文件使用，则尽量使用static关键字修饰；否则需要为全局变量定义赋初值(哪怕0值)，保证该变量为强符号，以便链接时发现变量名冲突，而不是被未知值覆盖。

  

某些编译器将未初始化的全局变量保存在common段，链接时再将其放入BSS段。在编译阶段可通过-fno-common选项来禁止将未初始化的全局变量放入common段。

  

- 堆(heap)

  

堆用于存放进程运行时动态分配的内存段，可动态扩张或缩减。堆中内容是匿名的，不能按名字直接访问，只能通过指针间接访问。当进程调用malloc(C)/new(C++)等函数分配内存时，新分配的内存动态添加到堆上(扩张)；当调用free(C)/delete(C++)等函数释放内存时，被释放的内存从堆中剔除(缩减) 。

  

分配的堆内存是经过字节对齐的空间，以适合原子操作。堆管理器通过链表管理每个申请的内存，由于堆申请和释放是无序的，最终会产生内存碎片。堆内存一般由应用程序分配释放，回收的内存可供重新使用。若程序员不释放，程序结束时操作系统可能会自动回收。

  

堆的末端由break指针标识，当堆管理器需要更多内存时，可通过系统调用brk()和sbrk()来移动break指针以扩张堆，一般由系统自动调用。

  

使用堆时经常出现两种问题：1) 释放或改写仍在使用的内存(“内存破坏”)；2)未释放不再使用的内存(“内存泄漏”)。当释放次数少于申请次数时，可能已造成内存泄漏。泄漏的内存往往比忘记释放的数据结构更大，因为所分配的内存通常会圆整为下个大于申请数量的2的幂次(如申请212B，会圆整为256B)。

  

- 内存映射段(mmap)

  

此处，内核将硬盘文件的内容直接映射到内存, 任何应用程序都可通过Linux的mmap()系统调用请求这种映射。内存映射是一种方便高效的文件I/O方式， 因而被用于装载动态共享库。用户也可创建匿名内存映射，该映射没有对应的文件, 可用于存放程序数据。在 Linux中，若通过malloc()请求一大块内存，C运行库将创建一个匿名内存映射，而不使用堆内存。”大块” 意味着比阈值 MMAP_THRESHOLD还大，缺省为128KB，可通过mallopt()调整。

  

该区域用于映射可执行文件用到的动态链接库。在Linux 2.4版本中，若可执行文件依赖共享库，则系统会为这些动态库在从0x40000000开始的地址分配相应空间，并在程序装载时将其载入到该空间。在Linux 2.6内核中，共享库的起始地址被往上移动至更靠近栈区的位置。

  

从进程地址空间的布局可以看到，在有共享库的情况下，留给堆的可用空间还有两处：一处是从.bss段到0x40000000，约不到1GB的空间；另一处是从共享库到栈之间的空间，约不到2GB。这两块空间大小取决于栈、共享库的大小和数量。这样来看，是否应用程序可申请的最大堆空间只有2GB？事实上，这与Linux内核版本有关。在上面给出的进程地址空间经典布局图中，共享库的装载地址为0x40000000，这实际上是Linux kernel 2.6版本之前的情况了，在2.6版本里，共享库的装载地址已经被挪到靠近栈的位置，即位于0xBFxxxxxx附近，因此，此时的堆范围就不会被共享库分割成2个“碎片”，故kernel 2.6的32位Linux系统中，malloc申请的最大内存理论值在2.9GB左右。

  

- 栈(stack)

  

栈又称堆栈，由编译器自动分配释放，行为类似数据结构中的栈(先进后出)。堆栈主要有三个用途：

  

- 为函数内部声明的非静态局部变量(C语言中称“自动变量”)提供存储空间。

- 记录函数调用过程相关的维护性信息，称为栈帧(Stack Frame)或过程活动记录(Procedure Activation Record)。它包括函数返回地址，不适合装入寄存器的函数参数及一些寄存器值的保存。除递归调用外，堆栈并非必需。因为编译时可获知局部变量，参数和返回地址所需空间，并将其分配于BSS段。

- 临时存储区，用于暂存长算术表达式部分计算结果或alloca()函数分配的栈内内存。

  

持续地重用栈空间有助于使活跃的栈内存保持在CPU缓存中，从而加速访问。进程中的每个线程都有属于自己的栈。向栈中不断压入数据时，若超出其容量就会耗尽栈对应的内存区域，从而触发一个页错误。此时若栈的大小低于堆栈最大值RLIMIT_STACK(通常是8M)，则栈会动态增长，程序继续运行。映射的栈区扩展到所需大小后，不再收缩。

  

Linux中ulimit -s命令可查看和设置堆栈最大值，当程序使用的堆栈超过该值时, 发生栈溢出(Stack Overflow)，程序收到一个段错误(Segmentation Fault)。注意，调高堆栈容量可能会增加内存开销和启动时间。

  

堆栈既可向下增长(向内存低地址)也可向上增长, 这依赖于具体的实现。本文所述堆栈向下增长。

  

栈的大小在运行时由内核动态调整。

  

#### 代码思考

  

~~~c

   #include<stdio.h>

   #include<unistd.h>

   #include<errno.h>

   int g_val = 0;

   int main(){

     pid_t pid;

     pid = fork();

    if(pid < 0){

      perror("fork error\n");

    }

    else if(pid == 0){

      printf("this is child process...........\n");

      printf("g_val:%d g_val'adress: %p\n",g_val,&g_val);

    }

    else{

      printf("this is parent process..........\n");

      printf("g_val:%d g_val'adress: %p\n",g_val = 100,&g_val);

      sleep(5);

    }

    return 0;

}

  

~~~

  

g_val的值在父进程和子进程分别打印出不同的值，但是地址是相同的。思考这里说明了什么问题。

  

##### 内存描述符(mm_struct)

  

![这里写图片描述](https://img-blog.csdn.net/20170112101815302?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjY3Njg3NDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

  

~~~

//每个进程都只有一个内存描述符mm_struct。mm_struct是对进程的地址空间(虚拟内存)的描述。

//一个进程的虚拟空间中可能有多个虚拟区间，对这些虚拟空间的组织方式有两种，当虚拟区较少时采用单链表，由mmap指针指向这个链表，当虚拟区间多时采用红黑树进行管理，由mm_rb指向这棵树。因为程序中用到的地址常常具有局部性，因此，最近一次用到的虚拟区间很可能下一次还要用到，因此把最近用到的虚拟区间结构放到高速缓存，这个虚拟区间就由mmap_cache指向。

  

//虽然每个进程只有一个虚拟空间，但是这个虚拟空间可以被别的进程来共享。如：子进程共享父进程的地址空间，而mm_user和mm_count就对其计数。

  

//由于进程的虚拟空间及下属的虚拟区间有可能在不同的上下文中受到访问，而这些访问又必须互斥，所以在该结构中设置了用于P,V操作的信号量mmap_sem。

  

struct mm_struct {

  

    //指向线性区对象的链表头

    struct vm_area_struct * mmap;       /* list of VMAs */

    //指向线性区对象的红黑树

    struct rb_root mm_rb;

    //指向最近找到的虚拟区间

    struct vm_area_struct * mmap_cache; /* last find_vma result */

  

    //用来在进程地址空间中搜索有效的进程地址空间的函数

    unsigned long (*get_unmapped_area) (struct file *filp,

                unsigned long addr, unsigned long len,

                unsigned long pgoff, unsigned long flags);

  

       unsigned long (*get_unmapped_exec_area) (struct file *filp,

                unsigned long addr, unsigned long len,

                unsigned long pgoff, unsigned long flags);

  

    //释放线性区时调用的方法，          

    void (*unmap_area) (struct mm_struct *mm, unsigned long addr);

  

    //标识第一个分配文件内存映射的线性地址

    unsigned long mmap_base;        /* base of mmap area */

  
  

    unsigned long task_size;        /* size of task vm space */

    /*

     * RHEL6 special for bug 790921: this same variable can mean

     * two different things. If sysctl_unmap_area_factor is zero,

     * this means the largest hole below free_area_cache. If the

     * sysctl is set to a positive value, this variable is used

     * to count how much memory has been munmapped from this process

     * since the last time free_area_cache was reset back to mmap_base.

     * This is ugly, but necessary to preserve kABI.

     */

    unsigned long cached_hole_size;

  

    //内核进程搜索进程地址空间中线性地址的空间空间

    unsigned long free_area_cache;      /* first hole of size cached_hole_size or larger */

  

    //指向页表的目录

    pgd_t * pgd;

  

    //共享进程时的个数

    atomic_t mm_users;          /* How many users with user space? */

  

    //内存描述符的主使用计数器，采用引用计数的原理，当为0时代表无用户再次使用

    atomic_t mm_count;          /* How many references to "struct mm_struct" (users count as 1) */

  

    //线性区的个数

    int map_count;              /* number of VMAs */

  

    struct rw_semaphore mmap_sem;

  

    //保护任务页表和引用计数的锁

    spinlock_t page_table_lock;     /* Protects page tables and some counters */

  

    //mm_struct结构，第一个成员就是初始化的mm_struct结构，

    struct list_head mmlist;        /* List of maybe swapped mm's.  These are globally strung

                         * together off init_mm.mmlist, and are protected

                         * by mmlist_lock

                         */

  

    /* Special counters, in some configurations protected by the

     * page_table_lock, in other configurations by being atomic.

     */

  

    mm_counter_t _file_rss;

    mm_counter_t _anon_rss;

    mm_counter_t _swap_usage;

  

    //进程拥有的最大页表数目

    unsigned long hiwater_rss;  /* High-watermark of RSS usage */、

    //进程线性区的最大页表数目

    unsigned long hiwater_vm;   /* High-water virtual memory usage */

  

    //进程地址空间的大小，锁住无法换页的个数，共享文件内存映射的页数，可执行内存映射中的页数

    unsigned long total_vm, locked_vm, shared_vm, exec_vm;

    //用户态堆栈的页数，

    unsigned long stack_vm, reserved_vm, def_flags, nr_ptes;

    //维护代码段和数据段

    unsigned long start_code, end_code, start_data, end_data;

    //维护堆和栈

    unsigned long start_brk, brk, start_stack;

    //维护命令行参数，命令行参数的起始地址和最后地址，以及环境变量的起始地址和最后地址

    unsigned long arg_start, arg_end, env_start, env_end;

  

    unsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */

  

    struct linux_binfmt *binfmt;

  

    cpumask_t cpu_vm_mask;

  

    /* Architecture-specific MM context */

    mm_context_t context;

  

    /* Swap token stuff */

    /*

     * Last value of global fault stamp as seen by this process.

     * In other words, this value gives an indication of how long

     * it has been since this task got the token.

     * Look at mm/thrash.c

     */

    unsigned int faultstamp;

    unsigned int token_priority;

    unsigned int last_interval;

  

    //线性区的默认访问标志

    unsigned long flags; /* Must use atomic bitops to access the bits */

  

    struct core_state *core_state; /* coredumping support */

#ifdef CONFIG_AIO

    spinlock_t      ioctx_lock;

    struct hlist_head   ioctx_list;

#endif

#ifdef CONFIG_MM_OWNER

    /*

     * "owner" points to a task that is regarded as the canonical

     * user/owner of this mm. All of the following must be true in

     * order for it to be changed:

     *

     * current == mm->owner

     * current->mm != mm

     * new_owner->mm == mm

     * new_owner->alloc_lock is held

     */

    struct task_struct *owner;

#endif

  

#ifdef CONFIG_PROC_FS

    /* store ref to file /proc/<pid>/exe symlink points to */

    struct file *exe_file;

    unsigned long num_exe_file_vmas;

#endif

#ifdef CONFIG_MMU_NOTIFIER

    struct mmu_notifier_mm *mmu_notifier_mm;

#endif

#ifdef CONFIG_TRANSPARENT_HUGEPAGE

    pgtable_t pmd_huge_pte; /* protected by page_table_lock */

#endif

    /* reserved for Red Hat */

#ifdef __GENKSYMS__

    unsigned long rh_reserved[2];

#else

    /* How many tasks sharing this mm are OOM_DISABLE */

    union {

        unsigned long rh_reserved_aux;

        atomic_t oom_disable_count;

    };

  

    /* base of lib map area (ASCII armour) */

    unsigned long shlib_base;

#endif

};

~~~

  

**vm_area_struct**

  

内存描述符mm_struct指向整个地址空间，vm_area_struct只是指向了虚拟空间的一段。vm_area_struct是由双向链表链接起来的，它们是按照虚拟地址降序排序的，每个这样的结构都对应描述一个地址空间范围。之所以这样分隔是因为每个虚拟区间可能来源不同，有的可能来自可执行映像，有的可能来自共享库，而有的可能是动态内存分配的内存区，所以对于每个由vm_area_struct结构所描述的区间的处理操作和它前后范围的处理操作不同，因此linux把虚拟内存分割管理，并利用了虚拟内存处理例程vm_ops来抽象对不同来源虚拟内存的处理方法。

  

##### 内存获取代码

  

**Java的**

  

~~~

Runtime runtime = Runtime.getRuntime();

//获取已经申请的Java内存 long usedMemory=runtime.totalMemory() ;

//获取申请但未使用Java内存 long freeMemory = runtime.freeMemory();

  
  

//系统内存

public static class MemoryInfo implements Parcelable {

        /**

         * The available memory on the system.  This number should not

         * be considered absolute: due to the nature of the kernel, a significant

         * portion of this memory is actually in use and needed for the overall

         * system to run well.

         */

        public long availMem;

  

        /**

         * The total memory accessible by the kernel.  This is basically the

         * RAM size of the device, not including below-kernel fixed allocations

         * like DMA buffers, RAM for the baseband CPU, etc.

         */

        public long totalMem;

  

        /**

         * The threshold of {@link #availMem} at which we consider memory to be

         * low and start killing background services and other non-extraneous

         * processes.

         */

        public long threshold;

  

        /**

         * Set to true if the system considers itself to currently be in a low

         * memory situation.

         */

        public boolean lowMemory;

  

~~~

  

**Native的**

  

![image-20230207141604798](/Users/jesson/Library/Application Support/typora-user-images/image-20230207141604798.png)

  

~~~

https://www.onitroad.com/jc/linux/man-pages/linux/man3/mallinfo.3.html

~~~

  

### 常见开源工具

  

https://github.com/Tencent/loli_profiler/blob/master/docs/QUICK_START_CN.md

  
  
  

#### Koom

  

![Koom-内存监测](/Users/jesson/Desktop/Koom-内存监测.png)

  

~~~shell

###### 内存相关

最大内存：javaHeap.max = Runtime.getRuntime().maxMemory()

  

总内存：javaHeap.total = Runtime.getRuntime().totalMemory()

  

空闲内存：javaHeap.free = Runtime.getRuntime().freeMemory()

  

使用内存=最大内存-空闲内存：javaHeap.used = javaHeap.total - javaHeap.free

  

使用占比=使用内存/最大内存：javaHeap.rate = 1.0f * javaHeap.used / javaHeap.max

  

###### Thread 获取

读取/proc/self/status 文件获取 Thread 线程数

###### FD 获取

读取/proc/self/fd 文件数

###### PhysicalMemory 获取

读取 /proc/meminfo 文件 获取内存信息

~~~

  

#### Koom给我们的总结和反思

  

- 主要可能发生OOM的场景：

  

  ~~~

  （1）堆内存溢出；这个是典型的OOM场景；【思路是计算出Heap的占用率 JVM中=-xms/-xmx】

  （2）没有连续的内存空间分配；这个主要是因为内存碎片过多（标记清除算法），导致即便内存够用，也会造成OOM；【获取/proc/self/status的Thread的count】

  （3）打开过多的文件；如果有碰到这个异常OOM：open to many file的伙伴，应该就知道了；

  （4）虚拟内存空间不足；

  （5）开启过多的线程；一般情况下，开启一个线程大概会分配500k的内存，如果开启线程过多同样会导致OOM

  ~~~

  

  

- ```

  private val mOOMTrackers = mutableListOf(

    HeapOOMTracker(), ThreadOOMTracker(), FdOOMTracker(),

    PhysicalMemoryOOMTracker(), FastHugeMemoryOOMTracker()

  )

  ```

  

- 关于Dump的操作为什么不能放在子线程

  

  ~~~

  虽然是在子线程内，但是还是会产生内存垃圾（一边采集数据，一边申请内存也不合理），还是需要GC去STW清理，如果放在单独的进程中，就不会加快主进程的GC，也是尽可能避免在dump时发生崩溃影响主进程。

  ~~~

  

#### 关于Dump的核心

  

~~~

private fun dumpAndAnalysis() {

    MonitorLog.i(TAG, "dumpAndAnalysis");

    runCatching {

      if (!OOMFileManager.isSpaceEnough()) {

        MonitorLog.e(TAG, "available space not enough", true)

        return@runCatching

      }

      if (mHasDumped) {

        return

      }

      mHasDumped = true

  

      val date = Date()

  

      val jsonFile = OOMFileManager.createJsonAnalysisFile(date)

      val hprofFile = OOMFileManager.createHprofAnalysisFile(date).apply {

        createNewFile()

        setWritable(true)

        setReadable(true)

      }

  

      MonitorLog.i(TAG, "hprof analysis dir:$hprofAnalysisDir")

  

      ForkJvmHeapDumper.getInstance().run {

        dump(hprofFile.absolutePath)

      }

  

      MonitorLog.i(TAG, "end hprof dump", true)

      Thread.sleep(1000) // make sure file synced to disk.

      MonitorLog.i(TAG, "start hprof analysis")

  

      startAnalysisService(hprofFile, jsonFile, mTrackReasons.joinToString())

    }.onFailure {

      it.printStackTrace()

  

      MonitorLog.i(TAG, "onJvmThreshold Exception " + it.message, true)

    }

  }

~~~

  

**其实fork进程前是不需要挂起子线程的**

  

**。**

  

https://github.com/Lingcc/AndroidArt/blob/e28ad4b915/runtime/debugger.cc

  

https://github.com/Lingcc/AndroidArt/blob/e28ad4b915/runtime/debugger.cc

  

~~~

nm -a libart.so > show.txt

~~~

  

#### Tracing_garbage_collection

  

https://github.com/khadas/android_system_memory_libmemunreachable/blob/khadas-vim4-r/Leak.h

  

https://github.com/bytedance/memory-leak-detector/blob/master/library/src/main/inline64/And64InlineHook.cpp

https://github.com/aosp-mirror/platform_bionic/blob/master/libc/bionic/pthread_attr.cpp

  
  
  

+++随笔+++

1、asm 插桩 gradle transform API

  

2、我们具备应付KPI的能力（内存）

  

内存的 计算的模型（Java/Native）Debug OS

  
  
  

#### 泄漏

  

C++ (4个函数) Xhook

  

Java的 （4大引用）GCRoot

  

SOftRef ：软引用  WeakRef 弱引用 ====》 判读对象是否被回收的准则

  
  
  

### 面试综合性问题的回答思路

  

APM 构建 搭建

  

1、指标（采集 瞬时的内存的暴增、TotalMem 80%）

  

2、监测的方案 （策略问题）

  

- thread 【/proc/self/status】

- fd [cd /proc/self/fd] 连续3次监测FD 如果>1000 又问题

- 泄漏的问题/阈值/

- physicalMemory 【本质： 文件读取】

  

3、Dump内存

  

Linux COW fork 子进程 和父进程 共享了进程的内存空间 需要做suspend vm ===》 so 库==〉7.0 ？？

  

内存的监测策略===>子进程内存的Dump[不阻塞主进程]===》shark库====> json

  

Koom 利用了 Linux COW fork了子进程dump的内存镜像，fork ok后==》parent process 恢复虚拟机运行

  

请大家复习一下 COW fork exec 进程的基本理论

  

4、内存优化的总结：

  

- Bitmap的优化

  - 链路回收 【统一的控制策略】

  - 大图的监控、重复图片【gradle】**TopN**

- 内存的监控

  - Java 【systrace、LeakCanary、MAT】

  - OOM的监控 【Koom】

  - Native的 【子杰、美团 、google的】

- 架构优化

  - APM  机器的分类

  - 缓存机制 【ComponentCallbacks2】

  - 进程减少 【空进程==10M】

  - so的优化 80mapk so文件 自己写的 编译的时候 -finstrument-functions：主要解决的内存的泄漏问题

  

5、指标的问题

  

- 内存的异常率: 主要采集的是PSS  PSS超过400M / 采集的 大小

- 触顶率Java内存的使用情况 平均数【Java native 图片 】

- GC ：

  

~~~

getGlobalAllocCount

getGlobalAllocSize

getRuntimeStat("art.gc.gc-count") //GC 的次数

  

~~~

  

- 内存的事件

  

~~~

接口

~~~